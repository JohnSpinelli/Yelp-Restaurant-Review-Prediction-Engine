{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Restaurants rating based on review text of Yelp reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To be completed INDIVIDUALLY and due on April 18 at 3pm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('John Spinelli, U50128653')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (50 pts) **\n",
    "\n",
    "In this assignment, we will be working with the [Yelp dataset](http://cs-people.bu.edu/bahargam/506/yelp_dataset_challenge_academic_dataset.tar). You can find the format of the dataset [here](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "Our aim is to predict the rating of each restaurant from the reviews **text** of individuals reviews. \n",
    "\n",
    "You can use any model you learned in the class and then evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM test set: 0.595\n",
      "Accuracy of SVM test set: 0.701175293823\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr 13 01:12:16 2017\n",
    "\n",
    "@author: JohnSpinelli\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import re\n",
    "import json\n",
    "from pandas import DataFrame as Df\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import sklearn.svm as svm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "path = \"/Users/JohnSpinelli/desktop/yelp_academic_dataset_business.json\"\n",
    "path2 = \"/Users/JohnSpinelli/desktop/yelp_academic_dataset_review.json\"\n",
    "\n",
    "### Explain Hashing Vectorizer, and site the source of clean review\n",
    "\n",
    "def roundRating(ratings):\n",
    "    return [round(ratings[x] * 2) / 2 for x in range(len(ratings))]\n",
    "\n",
    "def clean_review(review):\n",
    "    \"\"\"\n",
    "    Function to clean review text to keep only letters and remove stopwords\n",
    "    Returns a string of the cleaned bill text\n",
    "    \"\"\"\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    words = letters_only.lower().split()\n",
    "    stopwords_eng = set(stopwords.words(\"english\"))\n",
    "    useful_words = [x for x in words if not x in stopwords_eng]\n",
    "    \n",
    "    # Combine words into a paragraph again\n",
    "    useful_words_string = ' '.join(useful_words)\n",
    "    return(useful_words_string)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_words(words_list, stemmer):\n",
    "    return [stemmer.stem(word) for word in words_list]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_words(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "count = 0\n",
    "restaurantData = []\n",
    "categories = []\n",
    "\n",
    "with open(path,encoding = \"ISO-8859-1\") as jsonData:\n",
    "    for line in jsonData:\n",
    "        if count == 5000:\n",
    "            break\n",
    "        \n",
    "        data_contents = json.loads(line)\n",
    "        \n",
    "        if \"Restaurants\" in data_contents[\"categories\"]:\n",
    "            restaurantData.append({\"business_id\":data_contents[\"business_id\"],\\\n",
    "                                   'stars':data_contents['stars']})\n",
    "            count = count + 1\n",
    "            \n",
    "\n",
    "reviews = []\n",
    "count = 0\n",
    "with open(path2,encoding = \"ISO-8859-1\") as jsonData:\n",
    "    for line in jsonData:\n",
    "        \n",
    "        \n",
    "        line = json.loads(line)\n",
    "        reviews.append({'business_id':line['business_id'],'text':line['text']})\n",
    "        count = count + 1\n",
    "\n",
    "\n",
    "restaurantDf = Df(restaurantData)\n",
    "reviewDf = Df(reviews)\n",
    "\n",
    "mergedData = pd.merge(left=restaurantDf,right=reviewDf,on='business_id',how='inner') \n",
    "finalData = mergedData.drop_duplicates(\"business_id\")\n",
    "\n",
    "finalData['clean_review'] = finalData['text'].apply(clean_review)\n",
    "finalData['stars'] = np.round(finalData['stars'].values)\n",
    "finalData['stars'] = finalData['stars'].values.astype(int)\n",
    "vectorizer = HashingVectorizer(decode_error='ignore', n_features = 2**18,\n",
    "                               tokenizer = tokenize, non_negative=True)\n",
    "\n",
    "\n",
    "\n",
    "reviewVec = vectorizer.transform(finalData['clean_review'])\n",
    "stars = finalData['stars']\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviewVec, stars, test_size=0.2,random_state=42)\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred_test = svc.predict(X_test)\n",
    "print(\"Accuracy of SVM test set:\", svc.score(X_test, y_test))\n",
    "print(\"Accuracy of SVM test set:\", svc.score(X_train, y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean_review method as well as the decision to use a hashing vectorizer instead of a tfidvectorizer was inspired from online source code that aimed to solve a similar problem involving yelp reviews. https://beckernick.github.io/yelp-reviews/ \n",
    "\n",
    "As you can see from the results the accuracy is fairly good after we clean the reviews by stripping them for non-important words as well as stemming. I believe the choice of the SVM linear classifier is the best choice here and is widely used in many attempts to solve this challenge, an alternative option would have been a Naive Bayes Classifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
